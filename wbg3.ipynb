{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMalKYnjK3prUDXgj5mRiAF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1sSiTrn5iMix"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Load the Two Datasets ---\n",
        "\n",
        "# IMPORTANT: Ensure these filenames match your EXPORTED CSV files.\n",
        "projects_file = 'project_list.csv'\n",
        "outcomes_file = 'WBG_merged.csv'\n",
        "\n",
        "print(f\"Attempting to load Projects data from: {projects_file}\")\n",
        "print(f\"Attempting to load Outcomes data from: {outcomes_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_OTXrNHjJ_A",
        "outputId": "c77a9f7c-3b05-437f-ac07-3cd80b28193b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load Projects data from: project_list.csv\n",
            "Attempting to load Outcomes data from: WBG_merged.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- V V V --- VERIFY/ADJUST LOADING PARAMETERS BELOW --- V V V ---\n",
        "# Assuming header is on row 1 (the second row). Verify this!\n",
        "# Also verify the delimiter (',' or ';' or '\\t').\n",
        "\n",
        "try:\n",
        "    # --- Parameters for project_list.csv ---\n",
        "    projects_delimiter = ','   # Verify this\n",
        "    projects_header_row = 1   # Verify this (row 0=first, 1=second, etc.)\n",
        "    projects_skiprows = None\n",
        "\n",
        "    print(f\"\\nLoading {projects_file} with: delimiter='{projects_delimiter}', header={projects_header_row}, skiprows={projects_skiprows}\")\n",
        "    df_projects = pd.read_csv(\n",
        "        projects_file,\n",
        "        header=projects_header_row,\n",
        "        delimiter=projects_delimiter,\n",
        "        skiprows=projects_skiprows\n",
        "    )\n",
        "    print(f\"Loaded {projects_file} successfully. Shape: {df_projects.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ERROR LOADING {projects_file} ---\"); print(f\"Error message: {e}\"); exit()\n",
        "\n",
        "try:\n",
        "    # --- Parameters for WBG_merged.csv ---\n",
        "    outcomes_delimiter = ','   # Verify this\n",
        "    outcomes_header_row = 1   # Verify this\n",
        "    outcomes_skiprows = None\n",
        "\n",
        "    print(f\"\\nLoading {outcomes_file} with: delimiter='{outcomes_delimiter}', header={outcomes_header_row}, skiprows={outcomes_skiprows}\")\n",
        "    df_outcomes = pd.read_csv(\n",
        "        outcomes_file,\n",
        "        header=outcomes_header_row,\n",
        "        delimiter=outcomes_delimiter,\n",
        "        skiprows=outcomes_skiprows\n",
        "    )\n",
        "    print(f\"Loaded {outcomes_file} successfully. Shape: {df_outcomes.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ERROR LOADING {outcomes_file} ---\"); print(f\"Error message: {e}\"); exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XCTmdzGjNh1",
        "outputId": "c0bf25ed-add5-435c-d268-0a5ec316c4bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading project_list.csv with: delimiter=',', header=1, skiprows=None\n",
            "Loaded project_list.csv successfully. Shape: (445, 16)\n",
            "\n",
            "Loading WBG_merged.csv with: delimiter=',', header=1, skiprows=None\n",
            "Loaded WBG_merged.csv successfully. Shape: (166, 34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Clean and Standardize Column Names & Define Variables ---\n",
        "\n",
        "def standardize_cols(df):\n",
        "    \"\"\"Converts column names to lowercase, replaces spaces/special chars with underscores.\"\"\"\n",
        "    df.columns = df.columns.astype(str).str.strip().str.lower()\n",
        "    df.columns = df.columns.str.replace(' ', '_', regex=False)\n",
        "    df.columns = df.columns.str.replace(r'[ /$.]+', '_', regex=True)\n",
        "    df.columns = df.columns.str.replace(r'[^a-z0-9_]+', '', regex=True)\n",
        "    df.columns = df.columns.str.strip('_')\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Vi4SGDt1jQ8x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply standardization\n",
        "df_projects = standardize_cols(df_projects.copy())\n",
        "df_outcomes = standardize_cols(df_outcomes.copy())\n",
        "\n",
        "# Print standardized names for verification by user\n",
        "print(\"\\n--- Standardized Column Names ---\")\n",
        "print(\"Projects Dataset Columns (standardized):\", df_projects.columns.tolist())\n",
        "print(\"Outcomes Dataset Columns (standardized):\", df_outcomes.columns.tolist())\n",
        "print(\"--- >>>> Please verify the variable assignments below match these lists <<< ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H-RP7sPjTtz",
        "outputId": "c6885783-0219-45fa-a1d2-12863b878fe5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Standardized Column Names ---\n",
            "Projects Dataset Columns (standardized): ['project_id', 'region', 'country', 'project_name', 'project_url', 'year', 'month', 'day', 'sum_of_grant_amount__us', 'last_stage_reached', 'financing_type', 'effective_date', 'year_1', 'quarter', 'month_1', 'day_1']\n",
            "Outcomes Dataset Columns (standardized): ['as_of_date', 'project_id', 'project_name_x', 'wb_region', 'country___economy', 'country___economy_fcs_lending_group', 'country___economy_lending_group', 'practice_group', 'global_practice', 'agreement_type', 'lending_instrument_type', 'sum_of_approval_fy', 'sum_of_final_closing_fy', 'evaluation_type', 'outcome', 'outcome_1', 'quality_at_entry', 'quality_of_supervision', 'bank_performance', 'sum_of_evaluation_fy', 'region', 'country', 'project_name_y', 'project_url', 'year', 'month', 'day', 'last_stage_reached', 'financing_type', 'effective_date', 'year_1', 'quarter', 'month_1', 'day_1']\n",
            "--- >>>> Please verify the variable assignments below match these lists <<< ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Variable Assignments (Focus on H3 Relevant Columns) ---\n",
        "\n",
        "# Project ID (Merge Key)\n",
        "project_id_col_projects = 'project_id'\n",
        "project_id_col_outcomes = 'project_id'\n",
        "\n",
        "# Outcome Column (Using 'outcome_1' for text ratings)\n",
        "outcome_col = 'outcome_1'\n",
        "print(f\"\\nUsing '{outcome_col}' as the outcome column (for text ratings).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbFaJ-4OjYnw",
        "outputId": "c056ddc5-3184-4657-bdec-2cf26b8945d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using 'outcome_1' as the outcome column (for text ratings).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- List of Project Type Candidates for H3 ---\n",
        "# Add or remove standardized column names here based on your data and desired tests\n",
        "# Ensure these names EXACTLY match the standardized lists printed above.\n",
        "project_type_candidates = [\n",
        "    'lending_instrument_type', # From Outcomes\n",
        "    'practice_group',          # From Outcomes\n",
        "    'global_practice',         # From Outcomes\n",
        "    'financing_type'           # From Projects\n",
        "]\n",
        "print(f\"\\nPotential Project Type columns for H3 analysis: {project_type_candidates}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbFARLWHjbSn",
        "outputId": "f010e02a-d7f9-48a4-cda8-f317e20683e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Potential Project Type columns for H3 analysis: ['lending_instrument_type', 'practice_group', 'global_practice', 'financing_type']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Prepare for Merge: Standardize Merge Key ---\n",
        "merge_key = 'project_id_merged'\n",
        "if project_id_col_projects in df_projects.columns: df_projects.rename(columns={project_id_col_projects: merge_key}, inplace=True)\n",
        "else: print(f\"\\nError: Column '{project_id_col_projects}' not found in Projects DF.\"); exit()\n",
        "if project_id_col_outcomes in df_outcomes.columns: df_outcomes.rename(columns={project_id_col_outcomes: merge_key}, inplace=True)\n",
        "else: print(f\"\\nError: Column '{project_id_col_outcomes}' not found in Outcomes DF.\"); exit()\n",
        "print(f\"Renamed project ID columns to '{merge_key}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fekW7J6bjeUk",
        "outputId": "e43ce0e9-2e46-4062-cc96-0b726ddabc87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed project ID columns to 'project_id_merged'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ONLY columns needed for H3 analysis\n",
        "essential_cols_needed = [outcome_col] + project_type_candidates # Outcome and all type candidates\n",
        "essential_locations = {}\n",
        "\n",
        "# Check each essential column's location\n",
        "if outcome_col in df_outcomes.columns: essential_locations[outcome_col] = 'outcomes'\n",
        "# Check for project type candidates\n",
        "for pt_col in project_type_candidates:\n",
        "    if pt_col in df_outcomes.columns: essential_locations[pt_col] = 'outcomes'\n",
        "    elif pt_col in df_projects.columns: essential_locations[pt_col] = 'projects'\n",
        "\n",
        "# Verify all essential columns were located\n",
        "found_cols = list(essential_locations.keys())\n",
        "missing_essential = [col for col in essential_cols_needed if col not in found_cols]\n",
        "if missing_essential:\n",
        "    print(f\"\\nError: Could not locate all essential analysis columns needed for H3:\")\n",
        "    print(f\"  Missing: {missing_essential}\")\n",
        "    print(f\"  Check the variable assignments and candidate list match the standardized lists exactly.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(\"\\nVerified essential analysis columns locations:\")\n",
        "    #for col, source in essential_locations.items(): print(f\"  '{col}' found in {source} dataset.\") # Optional detail\n",
        "\n",
        "# Define columns to select for merge (merge key + located essential columns for H3)\n",
        "essential_cols_projects = list(set([merge_key] + [col for col, source in essential_locations.items() if source == 'projects']))\n",
        "essential_cols_outcomes = list(set([merge_key] + [col for col, source in essential_locations.items() if source == 'outcomes']))\n",
        "\n",
        "print(f\"\\nColumns intended to be kept from Projects: {essential_cols_projects}\")\n",
        "print(f\"Columns intended to be kept frm Outcomes: {essential_cols_outcomes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4PurIWBji7G",
        "outputId": "4b6d3bcc-05dc-43d5-9d0c-2e5e4c86e52a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verified essential analysis columns locations:\n",
            "\n",
            "Columns intended to be kept from Projects: ['project_id_merged']\n",
            "Columns intended to be kept frm Outcomes: ['outcome_1', 'global_practice', 'practice_group', 'lending_instrument_type', 'financing_type', 'project_id_merged']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Merge the Datasets ---\n",
        "\n",
        "cols_to_keep_projects = essential_cols_projects\n",
        "cols_to_keep_outcomes = essential_cols_outcomes\n",
        "print(f\"\\nSelecting columns for merge...\")\n",
        "\n",
        "df_merged = pd.merge(df_projects[cols_to_keep_projects], df_outcomes[cols_to_keep_outcomes], on=merge_key, how='inner')\n",
        "\n",
        "print(f\"\\nMerged DataFrame shape: {df_merged.shape}\")\n",
        "print(f\"DEBUG: Columns present in df_merged: {df_merged.columns.tolist()}\")\n",
        "if df_merged.empty: print(\"\\nError: Merged DataFrame is empty.\"); exit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keJgyCoGjoH1",
        "outputId": "b2ba5866-5f43-4201-d95c-0f337aac7177"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selecting columns for merge...\n",
            "\n",
            "Merged DataFrame shape: (166, 6)\n",
            "DEBUG: Columns present in df_merged: ['project_id_merged', 'outcome_1', 'global_practice', 'practice_group', 'lending_instrument_type', 'financing_type']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Preprocess Merged Data (Focus on H3 Columns) ---\n",
        "\n",
        "# Handle missing values in H3 essential columns\n",
        "print(\"\\nMissing values in merged data (before handling):\")\n",
        "key_analysis_cols_h3 = [outcome_col] + project_type_candidates # Check NAs for outcome and all type candidates\n",
        "print(df_merged[key_analysis_cols_h3].isnull().sum())\n",
        "\n",
        "rows_before_drop = df_merged.shape[0]\n",
        "# Drop rows where ANY H3 key analysis column is missing\n",
        "df_merged.dropna(subset=key_analysis_cols_h3, inplace=True)\n",
        "rows_after_drop = df_merged.shape[0]\n",
        "print(f\"\\nDropped {rows_before_drop - rows_after_drop} rows due to missing values in H3 key columns.\")\n",
        "print(f\"Merged DataFrame shape after dropping NAs: {df_merged.shape}\")\n",
        "if df_merged.empty: print(\"\\nError: DataFrame empty after dropping NAs for H3 columns.\"); exit()\n",
        "\n",
        "# Analyze and Binarize the outcome column ('outcome_1') using TEXT mapping\n",
        "print(f\"\\nUnique values found in outcome column '{outcome_col}' (expected text):\")\n",
        "df_merged[outcome_col] = df_merged[outcome_col].astype(str).str.strip()\n",
        "unique_outcomes = df_merged[outcome_col].unique()\n",
        "print(unique_outcomes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ip14bmfjrjL",
        "outputId": "d2489290-ad93-470b-aa0c-f8d7ff336dd6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values in merged data (before handling):\n",
            "outcome_1                  0\n",
            "lending_instrument_type    0\n",
            "practice_group             0\n",
            "global_practice            0\n",
            "financing_type             0\n",
            "dtype: int64\n",
            "\n",
            "Dropped 0 rows due to missing values in H3 key columns.\n",
            "Merged DataFrame shape after dropping NAs: (166, 6)\n",
            "\n",
            "Unique values found in outcome column 'outcome_1' (expected text):\n",
            "['Unsatisfactory' 'Satisfactory' 'Moderately Satisfactory'\n",
            " 'Moderately Unsatisfactory' 'Highly Satisfactory' 'Highly Unsatisfactory']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# *** REVIEW UNIQUE OUTCOMES ABOVE AND ADJUST MAPPING LISTS IF NEEDED ***\n",
        "satisfactory_ratings = ['Satisfactory', 'Moderately Satisfactory', 'Highly Satisfactory', 'SA', 'MS', 'HS']\n",
        "unsatisfactory_ratings = ['Unsatisfactory', 'Moderately Unsatisfactory', 'Highly Unsatisfactory', 'U', 'MU', 'HU']\n",
        "\n",
        "def map_text_outcome(outcome_text):\n",
        "    if outcome_text in unsatisfactory_ratings: return 1\n",
        "    elif outcome_text in satisfactory_ratings: return 0\n",
        "    else: return np.nan\n",
        "df_merged['outcome_binary'] = df_merged[outcome_col].apply(map_text_outcome)\n",
        "\n",
        "print(f\"\\nOutcome mapping summary (using text ratings from '{outcome_col}'):\")\n",
        "print(f\"  Mapped to 0 (Satisfactory): { (df_merged['outcome_binary'] == 0).sum() }\")\n",
        "print(f\"  Mapped to 1 (Unsatisfactory): { (df_merged['outcome_binary'] == 1).sum() }\")\n",
        "unmapped_values = df_merged[df_merged['outcome_binary'].isnull()][outcome_col].unique()\n",
        "print(f\"  Became NaN (unmapped: {unmapped_values}): { df_merged['outcome_binary'].isnull().sum() }\")\n",
        "if df_merged['outcome_binary'].isnull().sum() > 0: print(\"  >> If unmapped values exist, update satisfactory/unsatisfactory_ratings lists! <<\")\n",
        "\n",
        "df_merged.dropna(subset=['outcome_binary'], inplace=True)\n",
        "df_merged['outcome_binary'] = df_merged['outcome_binary'].astype(int)\n",
        "\n",
        "print(f\"\\nFinal shape after outcome mapping: {df_merged.shape}\")\n",
        "print(\"Value counts for binary outcome (1=Unsatisfactory, 0=Satisfactory):\")\n",
        "print(df_merged['outcome_binary'].value_counts(dropna=False))\n",
        "if df_merged['outcome_binary'].nunique() < 2: print(f\"\\nError: Only one outcome category exists after text mapping.\"); exit()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL8JKZQwjwQ6",
        "outputId": "f5410360-83d5-47b1-8563-1b59d0f166ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Outcome mapping summary (using text ratings from 'outcome_1'):\n",
            "  Mapped to 0 (Satisfactory): 117\n",
            "  Mapped to 1 (Unsatisfactory): 49\n",
            "  Became NaN (unmapped: []): 0\n",
            "\n",
            "Final shape after outcome mapping: (166, 7)\n",
            "Value counts for binary outcome (1=Unsatisfactory, 0=Satisfactory):\n",
            "outcome_binary\n",
            "0    117\n",
            "1     49\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Hypothesis Testing (H3 Only) ---\n",
        "\n",
        "print(\"\\n--- Hypothesis Testing (H3 Only) ---\")\n",
        "alpha = 0.05\n",
        "\n",
        "# --- H3 (Loop): Testing different Project Type columns vs Outcome ---\n",
        "print(f\"\\n--- H3 (Loop): Testing Potential Project Type Columns vs. Outcome ---\")\n",
        "\n",
        "# Loop through the candidate columns defined in Step 2\n",
        "for candidate_col in project_type_candidates:\n",
        "    print(f\"\\n--- H3 Testing: '{candidate_col}' vs. Outcome ---\")\n",
        "\n",
        "    # Check if column exists and has enough unique values in the final data\n",
        "    if candidate_col not in df_merged.columns:\n",
        "        print(f\"Skipping H3 test: Column '{candidate_col}' not found in final merged/cleaned data.\")\n",
        "        continue\n",
        "    # Also check for sufficient variation after NA removal for this specific column\n",
        "    if df_merged[candidate_col].nunique() < 2:\n",
        "        print(f\"Skipping H3 test: Column '{candidate_col}' has less than 2 unique values after cleaning.\")\n",
        "        continue\n",
        "\n",
        "    # Create contingency table for this candidate\n",
        "    contingency_table_h3 = pd.crosstab(df_merged[candidate_col], df_merged['outcome_binary'])\n",
        "\n",
        "    # Check if table is valid\n",
        "    if contingency_table_h3.shape[0] < 2 or contingency_table_h3.shape[1] < 2:\n",
        "         print(f\"Warning H3 ({candidate_col}): Contingency table too small for Chi-squared test (Shape: {contingency_table_h3.shape}).\")\n",
        "         continue # Skip to next candidate in the loop\n",
        "\n",
        "    # Perform the Chi-Squared test\n",
        "    try:\n",
        "        chi2_stat, p_value_h3, dof, expected_freq = stats.chi2_contingency(contingency_table_h3)\n",
        "        print(f\"Chi-Squared Statistic: {chi2_stat:.4f}, P-value: {p_value_h3:.4g}, DoF: {dof}\")\n",
        "\n",
        "        # Check assumption: Expected frequencies >= 5\n",
        "        low_freq_count = (expected_freq < 5).sum() if expected_freq is not None else -1\n",
        "        if expected_freq is not None and expected_freq.size > 0:\n",
        "            perc_low = (low_freq_count / expected_freq.size) * 100\n",
        "            if low_freq_count > 0: print(f\"Warning H3 ({candidate_col}): {low_freq_count} cells ({perc_low:.1f}%) have expected frequency < 5. Caution advised.\")\n",
        "            else: print(f\"Check H3 ({candidate_col}): Expected frequency assumption met.\")\n",
        "        else: print(f\"Warning H3 ({candidate_col}): Could not check expected frequencies.\")\n",
        "\n",
        "        # Interpretation\n",
        "        if p_value_h3 < alpha:\n",
        "            print(f\"Result H3 ({candidate_col}): Reject null hypothesis (p < {alpha}). Significant association between {candidate_col} and outcome.\")\n",
        "            contingency_table_h3['unsat_rate'] = contingency_table_h3[1] / (contingency_table_h3[0] + contingency_table_h3[1])\n",
        "            print(f\"\\nUnsatisfactory rates by {candidate_col}:\"); print(contingency_table_h3.sort_values('unsat_rate', ascending=False) if contingency_table_h3.shape[0] > 1 else contingency_table_h3)\n",
        "            # Optional Visualization for significant results\n",
        "            try:\n",
        "                if 0 in contingency_table_h3.columns and 1 in contingency_table_h3.columns:\n",
        "                    props = contingency_table_h3[[0, 1]].apply(lambda x: x/x.sum() if x.sum() > 0 else 0, axis=1)\n",
        "                    props.plot(kind='bar', stacked=True, figsize=(10, 6)); plt.title(f'Outcome Proportion by {candidate_col}')\n",
        "                    plt.xlabel(f'{candidate_col}'); plt.ylabel('Proportion'); plt.xticks(rotation=45, ha='right')\n",
        "                    plt.legend(['Satisfactory (0)', 'Unsatisfactory (1)'], title='Outcome'); plt.tight_layout(); plt.show()\n",
        "                else: print(f\"Could not generate plot for H3 ({candidate_col}): Missing outcome categories.\")\n",
        "            except Exception as e: print(f\"\\nCould not generate plot for H3 ({candidate_col}): {e}\")\n",
        "        else:\n",
        "            print(f\"Result H3 ({candidate_col}): Fail to reject null hypothesis (p >= {alpha}). No significant association between {candidate_col} and outcome.\")\n",
        "    except ValueError as ve:\n",
        "        print(f\"Error H3 ({candidate_col}): Chi-Squared test failed. ValueError: {ve}\")\n",
        "# --- (End of H3 loop) ---\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyEVH1YLj4F7",
        "outputId": "1de55958-fc5e-469d-8bbe-3289b32681bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Hypothesis Testing (H3 Only) ---\n",
            "\n",
            "--- H3 (Loop): Testing Potential Project Type Columns vs. Outcome ---\n",
            "\n",
            "--- H3 Testing: 'lending_instrument_type' vs. Outcome ---\n",
            "Chi-Squared Statistic: 1.3722, P-value: 0.5035, DoF: 2\n",
            "Warning H3 (lending_instrument_type): 3 cells (50.0%) have expected frequency < 5. Caution advised.\n",
            "Result H3 (lending_instrument_type): Fail to reject null hypothesis (p >= 0.05). No significant association between lending_instrument_type and outcome.\n",
            "\n",
            "--- H3 Testing: 'practice_group' vs. Outcome ---\n",
            "Chi-Squared Statistic: 6.5825, P-value: 0.1597, DoF: 4\n",
            "Warning H3 (practice_group): 2 cells (20.0%) have expected frequency < 5. Caution advised.\n",
            "Result H3 (practice_group): Fail to reject null hypothesis (p >= 0.05). No significant association between practice_group and outcome.\n",
            "\n",
            "--- H3 Testing: 'global_practice' vs. Outcome ---\n",
            "Chi-Squared Statistic: 14.6138, P-value: 0.3321, DoF: 13\n",
            "Warning H3 (global_practice): 15 cells (53.6%) have expected frequency < 5. Caution advised.\n",
            "Result H3 (global_practice): Fail to reject null hypothesis (p >= 0.05). No significant association between global_practice and outcome.\n",
            "\n",
            "--- H3 Testing: 'financing_type' vs. Outcome ---\n",
            "Chi-Squared Statistic: 1.0159, P-value: 0.6017, DoF: 2\n",
            "Check H3 (financing_type): Expected frequency assumption met.\n",
            "Result H3 (financing_type): Fail to reject null hypothesis (p >= 0.05). No significant association between financing_type and outcome.\n"
          ]
        }
      ]
    }
  ]
}