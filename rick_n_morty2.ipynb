{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkZGqSbufXvi6aY++BImf4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Part 2\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lNYJrg8oGGn",
        "outputId": "fe920b35-e49d-42ca-9b84-bf3fd84986f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "df = pd.read_csv(\"rick_n_morty.csv\")\n",
        "\n",
        "# Preprocessing: Aggregate dialogue per episode and calculate word count\n",
        "episode_dialogue = df.groupby(['Season No', 'episode no.'])['dialouge'].apply(' '.join).reset_index()\n",
        "episode_scores = df[['Season No', 'episode no.', 'IMDb score ']].drop_duplicates()\n",
        "episode_data = pd.merge(episode_dialogue, episode_scores, on=['Season No', 'episode no.'], how='left')\n"
      ],
      "metadata": {
        "id": "PEb2yt7QpGWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Text\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "episode_data['normalized_dialogue'] = episode_data['dialouge'].apply(normalize_text)\n"
      ],
      "metadata": {
        "id": "aYzFCMiFuDjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word count\n",
        "episode_data['word_count'] = episode_data['normalized_dialogue'].apply(lambda x: len(x.split()))\n"
      ],
      "metadata": {
        "id": "ZCSWaYN4uGiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer_tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
        "tfidf_matrix = vectorizer_tfidf.fit_transform(episode_data['normalized_dialogue'])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
        "episode_data = pd.concat([episode_data, tfidf_df], axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "vHQAhL1VpMCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec Embeddings\n",
        "sentences = [doc.split() for doc in episode_data['normalized_dialogue']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
        "\n",
        "def get_episode_embedding(episode_text):\n",
        "    words = episode_text.split()\n",
        "    vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(100)\n",
        "\n",
        "embeddings = np.vstack(episode_data['normalized_dialogue'].apply(get_episode_embedding))\n",
        "embedding_df = pd.DataFrame(embeddings, columns=[f'embedding_{i}' for i in range(100)])\n",
        "\n"
      ],
      "metadata": {
        "id": "w1TYLIIkpb1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA for Word2Vec Embeddings\n",
        "n_components = min(embedding_df.shape[0], embedding_df.shape[1])\n",
        "pca = PCA(n_components=n_components)\n",
        "pca_embeddings = pca.fit_transform(embedding_df)\n",
        "pca_embedding_df = pd.DataFrame(pca_embeddings, columns=[f'pca_embedding_{i}' for i in range(n_components)])\n",
        "episode_data = pd.concat([episode_data, pca_embedding_df], axis=1)\n"
      ],
      "metadata": {
        "id": "y68G8PYcpu_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "episode_data['sentiment_compound'] = episode_data['dialouge'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "episode_data['sentiment_variance'] = episode_data['dialouge'].apply(lambda x: np.var([analyzer.polarity_scores(s)['compound'] for s in sent_tokenize(x)]))\n"
      ],
      "metadata": {
        "id": "zg-CAyrTpfu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and target (EXCLUDE scene_changes)\n",
        "features = episode_data.drop(['Season No', 'episode no.', 'dialouge', 'IMDb score ', 'normalized_dialogue'], axis=1)\n",
        "target = episode_data['IMDb score ']\n"
      ],
      "metadata": {
        "id": "lAg2y8oqp9f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "b2XrWE7DqCu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg_-Sf9NrtuE",
        "outputId": "1a50c6ce-8dbf-48fc-fb5b-f95d6b01a4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_count              int64\n",
            "10                    float64\n",
            "137                   float64\n",
            "20                    float64\n",
            "22                    float64\n",
            "                       ...   \n",
            "pca_embedding_36      float32\n",
            "pca_embedding_37      float32\n",
            "pca_embedding_38      float32\n",
            "sentiment_compound    float64\n",
            "sentiment_variance    float64\n",
            "Length: 1042, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Regression\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "gb_predictions = gb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "GEyLSg8xqE83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "def evaluate_model(predictions, actual):\n",
        "    rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
        "    r2 = r2_score(actual, predictions)\n",
        "    mae = mean_absolute_error(actual, predictions)\n",
        "    return rmse, r2, mae\n",
        "\n",
        "gb_rmse, gb_r2, gb_mae = evaluate_model(gb_predictions, y_test)\n",
        "print(\"Gradient Boosting: RMSE={}, R2={}, MAE={}\".format(gb_rmse, gb_r2, gb_mae))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeY32wrUqH2k",
        "outputId": "b87cc24e-3abb-4f0b-92d9-b364d5a52cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting: RMSE=1.0693835641941645, R2=-0.05459938431687772, MAE=0.9623657388185116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "feature_importance = gb_model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(\"\\nFeature Importance:\")\n",
        "print(importance_df.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0bZ5O4dqO5C",
        "outputId": "4e94758e-1273-4548-e890-e30689db1126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importance:\n",
            "               Feature  Importance\n",
            "206               dumb    0.407257\n",
            "890               told    0.085949\n",
            "32              answer    0.082127\n",
            "16               ahead    0.076455\n",
            "970                win    0.050425\n",
            "781               sigh    0.022493\n",
            "918                 uh    0.022348\n",
            "25             alright    0.016301\n",
            "974              woman    0.016074\n",
            "1005   pca_embedding_4    0.014035\n",
            "961           whatever    0.012492\n",
            "1036  pca_embedding_35    0.012450\n",
            "1025  pca_embedding_24    0.010495\n",
            "158              crazy    0.009721\n",
            "238                eye    0.009483\n",
            "164                cry    0.008450\n",
            "676               pull    0.008231\n",
            "624            outside    0.008022\n",
            "989               yeah    0.007603\n",
            "224               ever    0.006864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Matrix\n",
        "correlation_matrix = features.corr()\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_XpzUUTqTQh",
        "outputId": "80140c12-34bc-4b26-c0a5-87d61633eb50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correlation Matrix:\n",
            "                    word_count        10       137        20        22  \\\n",
            "word_count            1.000000  0.107349  0.010049  0.091375  0.066154   \n",
            "10                    0.107349  1.000000 -0.027661  0.229401  0.053532   \n",
            "137                   0.010049 -0.027661  1.000000 -0.029378 -0.051897   \n",
            "20                    0.091375  0.229401 -0.029378  1.000000 -0.015271   \n",
            "22                    0.066154  0.053532 -0.051897 -0.015271  1.000000   \n",
            "...                        ...       ...       ...       ...       ...   \n",
            "pca_embedding_36     -0.419449  0.086669  0.057044  0.171725 -0.194125   \n",
            "pca_embedding_37     -0.086487 -0.109433  0.037969  0.376266 -0.152985   \n",
            "pca_embedding_38      0.027456  0.182810 -0.059788 -0.083147  0.308151   \n",
            "sentiment_compound    0.158783 -0.077784  0.171079  0.016670  0.112056   \n",
            "sentiment_variance    0.128563 -0.084297  0.133971 -0.028406  0.156967   \n",
            "\n",
            "                         aah      able      acid    across  actually  ...  \\\n",
            "word_count          0.195355  0.132010 -0.254648  0.001963 -0.040961  ...   \n",
            "10                 -0.067007  0.043515 -0.123567 -0.153836 -0.100285  ...   \n",
            "137                -0.118051  0.287054 -0.065358 -0.112544 -0.063929  ...   \n",
            "20                  0.066992  0.170328 -0.105292 -0.188193  0.018755  ...   \n",
            "22                 -0.024948 -0.082988 -0.042701  0.088846  0.016447  ...   \n",
            "...                      ...       ...       ...       ...       ...  ...   \n",
            "pca_embedding_36   -0.350652 -0.072906  0.003441 -0.170546  0.002425  ...   \n",
            "pca_embedding_37    0.024212  0.119239 -0.040129  0.004448  0.176233  ...   \n",
            "pca_embedding_38    0.034843  0.029324 -0.092942  0.224191  0.084998  ...   \n",
            "sentiment_compound  0.050010  0.163257 -0.408993  0.012151  0.137560  ...   \n",
            "sentiment_variance  0.081946 -0.036001  0.065158  0.355296  0.299093  ...   \n",
            "\n",
            "                    pca_embedding_31  pca_embedding_32  pca_embedding_33  \\\n",
            "word_count             -4.012207e-02     -1.592271e-01     -1.540350e-01   \n",
            "10                     -1.952866e-01     -5.197006e-02      2.179503e-01   \n",
            "137                    -8.144270e-02     -8.454516e-02     -4.841414e-03   \n",
            "20                     -1.269240e-01      1.508233e-01      1.891595e-01   \n",
            "22                      1.747889e-01     -5.877431e-02      8.875105e-02   \n",
            "...                              ...               ...               ...   \n",
            "pca_embedding_36       -6.776231e-08      1.214439e-07     -3.900113e-07   \n",
            "pca_embedding_37       -1.499502e-07     -3.616256e-07     -2.423225e-07   \n",
            "pca_embedding_38        1.136685e-01     -7.106664e-02      5.361937e-01   \n",
            "sentiment_compound     -6.439184e-02      1.910399e-01      3.093219e-02   \n",
            "sentiment_variance      4.470053e-02     -1.020441e-01     -4.470058e-02   \n",
            "\n",
            "                    pca_embedding_34  pca_embedding_35  pca_embedding_36  \\\n",
            "word_count             -1.260149e-01     -7.590566e-02     -4.194493e-01   \n",
            "10                     -1.193625e-01      6.576823e-02      8.666858e-02   \n",
            "137                     3.419957e-01     -2.957011e-01      5.704400e-02   \n",
            "20                     -3.480549e-01      2.881385e-01      1.717247e-01   \n",
            "22                     -1.874816e-01      2.701994e-01     -1.941251e-01   \n",
            "...                              ...               ...               ...   \n",
            "pca_embedding_36       -3.710915e-08      1.806106e-08      1.000000e+00   \n",
            "pca_embedding_37       -1.202053e-07     -1.264669e-08     -4.605871e-08   \n",
            "pca_embedding_38       -1.030092e-01      1.684644e-02     -5.735854e-02   \n",
            "sentiment_compound      5.719936e-02     -8.623088e-02     -2.453465e-01   \n",
            "sentiment_variance     -7.376861e-02      7.257688e-02      6.387959e-02   \n",
            "\n",
            "                    pca_embedding_37  pca_embedding_38  sentiment_compound  \\\n",
            "word_count             -8.648716e-02          0.027456            0.158783   \n",
            "10                     -1.094327e-01          0.182810           -0.077784   \n",
            "137                     3.796891e-02         -0.059788            0.171079   \n",
            "20                      3.762662e-01         -0.083147            0.016670   \n",
            "22                     -1.529849e-01          0.308151            0.112056   \n",
            "...                              ...               ...                 ...   \n",
            "pca_embedding_36       -4.605871e-08         -0.057359           -0.245347   \n",
            "pca_embedding_37        1.000000e+00         -0.681907            0.131108   \n",
            "pca_embedding_38       -6.819067e-01          1.000000           -0.041624   \n",
            "sentiment_compound      1.311077e-01         -0.041624            1.000000   \n",
            "sentiment_variance      6.521821e-02         -0.053203           -0.256711   \n",
            "\n",
            "                    sentiment_variance  \n",
            "word_count                    0.128563  \n",
            "10                           -0.084297  \n",
            "137                           0.133971  \n",
            "20                           -0.028406  \n",
            "22                            0.156967  \n",
            "...                                ...  \n",
            "pca_embedding_36              0.063880  \n",
            "pca_embedding_37              0.065218  \n",
            "pca_embedding_38             -0.053203  \n",
            "sentiment_compound           -0.256711  \n",
            "sentiment_variance            1.000000  \n",
            "\n",
            "[1042 rows x 1042 columns]\n"
          ]
        }
      ]
    }
  ]
}